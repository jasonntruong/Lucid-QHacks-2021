# QHacks 2021
Top 6 Finalist in for QHacks 2021 (Queen's University Hackathon) 

# Solution
Lucid aims to make life easier for the blind. It's main function is to use the phone's camera and text-to-speech to tell the user what objects are in front of them (object recognition) or what the text in front of them says (text-recognition). Ex: Grocery shopping, blind people can't tell if an apple is red or green!

We also have incredible UI/UX features built in like physical/tactile feedback on button clicks, text-to-speech to describe the activity that appears on the screen, voice command startup, speech speed customization, and many other features so that Lucid is perfectly curated for users without sight.

# Technologies Used
Backend: Using Google Cloud's Vision API, Machine Learning Kit (machine learning for mobile developers), Java, and Android Studio
Frontend: XML and Android Studio

# Links
DevPost: https://devpost.com/software/lucid-eyes-for-the-visually-impaired
Demo: https://www.youtube.com/watch?v=IddtCbX6l4Y
 
I have removed the Google Cloud Service Account associated with Lucid
